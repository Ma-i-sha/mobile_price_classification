# Mobile Price Classification

Predict mobile phone **price range** (0 = Low, 1 = Medium, 2 = High, 3 = Very High) from device specs.

This repo contains two notebooks:
- `Mobile_price.ipynb` â€” original version using **Logistic Regression**, **Decision Tree**, **kâ€‘NN**.
- `RF__Mobile_price_updated.ipynb` â€” updated version where **Decision Tree** is swapped with **Random Forest** (plus the same LR & kâ€‘NN).

Both notebooks train on **all 20 features** from `train.csv`. A postâ€‘analysis section (in the RF notebook variant with Topâ€‘15 cells) reports **feature importance** and optionally extracts a **Topâ€‘15** feature list (analysis only).

---

## ğŸš€ Quick Start

1. **Clone** and enter the repo
```bash
git clone <your-repo-url>.git
cd <your-repo-name>
```

2. **Install deps**
```bash
pip install -r requirements.txt
```

3. **Add data**
Place the Kaggle dataset files in the repo root:
```
train.csv
test.csv
```

4. **Run notebooks**
Open Jupyter and run either notebook endâ€‘toâ€‘end:
```bash
jupyter notebook
# then open Mobile_price.ipynb OR RF__Mobile_price_updated.ipynb
```

---

## ğŸ“¦ Data

Dataset: **Mobile Price Classification** (Kaggle).  
Target: `price_range` (0â€‘3) â€” balanced across classes.  
Features (20): battery_power, blue, clock_speed, dual_sim, fc, four_g, int_memory, m_dep, mobile_wt, n_cores, pc, px_height, px_width, ram, sc_h, sc_w, talk_time, three_g, touch_screen, wifi.

---

## ğŸ§  Models

- **Logistic Regression** (scaled features)
- **Decision Tree** (trees do not require scaling) â€” *original notebook only*
- **Random Forest** (swapâ€‘in for Decision Tree) â€” *updated notebook*
- **kâ€‘Nearest Neighbors** (scaled features)

**Train/Test Split:** 80/20 with `random_state=42` and stratification.

---

## âœ… Results (on heldâ€‘out test set)
From the run captured in the conversation:
- **Logistic Regression** â€” Accuracy: **0.975** (best, strong precision/recall across classes)
- **Random Forest** â€” Accuracy: **0.8925**
- **Decision Tree** â€” Accuracy: **0.8325**
- **kâ€‘NN** â€” Accuracy: **0.53**

> Confusion matrices and classification reports are produced by the notebooks.

---

## ğŸ” Feature Importance & Topâ€‘15 (Analysis Only)
The RF notebook includes a section *after* the confusion matrices that:
- Confirms the **20 features** used.
- Plots **impurityâ€‘based** importances (`rf_model.feature_importances_`).
- Computes **Permutation Importance** on the test set (modelâ€‘agnostic).
- Prints the **Topâ€‘15** list and (optionally) compares accuracy using only those features.

Why this matters:
- Helps justify why features like **RAM** and **battery_power** are important.
- Demonstrates that removing redundant/lowâ€‘signal features can retain accuracy (or even slightly improve it), though training remains on all 20 for the main results.

---

## ğŸ—‚ï¸ Repo Structure

```
.
â”œâ”€â”€ Mobile_price.ipynb
â”œâ”€â”€ RF__Mobile_price_updated.ipynb
â”œâ”€â”€ train.csv                # add locally (not tracked in git by default)
â”œâ”€â”€ test.csv                 # add locally (not tracked in git by default)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ” Reproducibility

- Fixed seeds where applicable (`random_state=42`).
- Scaling with `StandardScaler` for LR and kâ€‘NN uses **trainingâ€‘only fit**, then transforms test data (no leakage).

---

## ğŸ“œ License

This project is released under the **MIT License** (see `LICENSE`).

---

## ğŸ™Œ Acknowledgements

- Kaggle: Mobile Price Classification dataset.
- scikitâ€‘learn, pandas, numpy, matplotlib, seaborn.
